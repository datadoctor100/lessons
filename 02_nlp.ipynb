{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful Data Science Event:\n",
    "\n",
    "## Planning a Vacation with NLP\n",
    "\n",
    "The purpose of this event is to become familiar with natural language processing(NLP). \n",
    "\n",
    "NLP entails taking unstructured data such as text and transforming it into a useful format from which we can extract insights. \n",
    "\n",
    "In today's event, we will use this methodology in order to help us in planning and ideal vacation. The Kaggle website contains a file of hotel reviews which can be downloaded [here](https://www.kaggle.com/datafiniti/hotel-reviews/data). Our task is to use these reviews to create a random forest that will help plan our ideal location for a holiday.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started-\n",
    "\n",
    "The data for tonights event contains reviews for 1000 hotels collected by Datafinity. We are going to use these reviews to help us plan a vacation. Before we can do that, however, we will have to pre-process our data. This will include transforming the shape and structure of the data to make it readable by our algorithm, and also the removal of unnecessary information that cannot be interpreted by the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "In order to complete our project, we will rely on the `nltk` package which provides an excellent platform in python for carrying out natural language processing. To install this package and set it up:\n",
    "\n",
    "1. Open up a terminal window or command line:\n",
    "    * Run `pip install --upgrade pip`\n",
    "    * Then type `pip install nltk`\n",
    "    \n",
    "2. Create a new instance of python\n",
    "    * Run the following line: `import nltk`\n",
    "    * Then you must type in `nltk.download()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the last line, it will bring out a pop-up window that looks like this:\n",
    "\n",
    "![Image of NLTK](https://likegeeks.com/wp-content/uploads/2017/09/01-NLP-Tutorial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages are all small, so we can go ahead and install them all. Then simply close the pop-up and we can continue with our project in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing, model_selection, metrics, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the working directory\n",
    "# print(os.getcwd())\n",
    "# path = 'path/to/files'\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('https://github.com/Thinkful-Ed/data-201-resources/raw/master/hotel-reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing-\n",
    "\n",
    "This is often the most time consuming component of any data science project. Indeed, it can comprise up to 90% of the effort in building a model. \n",
    "\n",
    "In order to analyze our data, we must first process it through several tasks:\n",
    "* Remove casing\n",
    "* Separate words\n",
    "* Remove unnecessary information\n",
    "* Word \"stemming\"\n",
    "* TF-IDF\n",
    "* Encode the target variable\n",
    "* Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lower-case\n",
    "data['reviews.text'] = data['reviews.text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "data[\"words\"] = data['reviews.text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is referred to as \"tokenization\". This separates each word in the document or \"corpus\". \n",
    "\n",
    "Tokenization can take place at two different levels. \n",
    "\n",
    "First of all, we can separate sentences. More importantly, we can separate individual words. For our purposes, we will need to separate the words. \n",
    "\n",
    "For this reason, we can inherently ignore the sentence tokenization. Plus, we've already removed all the punctuation from our dataframe anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "data['words'] = data['reviews.text'].apply(str)\n",
    "\n",
    "data['words'] = data['words'].apply(lambda row: word_tokenize(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have separated all of our words, the data looks like this- for each row, the `reviews.text` column has been transformed from sentences into a simple list of words. \n",
    "\n",
    "We still need to further process these results. \n",
    "\n",
    "First, we must remove the \"stop words\". These are articles like \"the, is, are, etc.\" that don't really add any important information to the text. \n",
    "\n",
    "`NLTK` has lists of common stop words in many different languages. We can use these lists to automatically filter the stop words out of our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['words'] = data['words'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in processing our data is called \"stemming\". \n",
    "\n",
    "Basically, all this does is strip individual words down to their roots. For example, running would become run. \n",
    "\n",
    "The `NLTK` library also provides a convenient function for doing this as well. There are even several options that allow you to stem the words into different formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Stemming' the words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "data['stemmed'] = data['words'].apply(lambda x: [ps.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was mentioned earlier that our pre-processing left us with a column in which the data for each row was a list. \n",
    "\n",
    "In python(pandas), those lists are seen as objects. These objects will cause problems as we procede. \n",
    "\n",
    "Before continuing any further, we will go ahead and rejoin the lists. This will leave us with a single `string` of words separated by spaces for each row of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoin the words\n",
    "data['stemmed'] = data['stemmed'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['x'] = data['stemmed']\n",
    "\n",
    "df['y'] = data['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences, whether they are in the form of lists or paragraphs, cannot really be understood by machine learning algorithms. We still need to convert our reviews into a binary format in which words are represented by 0's and 1's. \n",
    "\n",
    "Basically, our columns will become words and our rows will be sentences. Then, for each sentence- the words contained will be represented as a 1 in the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectors for each review\n",
    "cv = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply counter to data\n",
    "x = cv.fit_transform(df['x'])\n",
    "\n",
    "x = pd.DataFrame(x.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the process is called tf-idf. This stands for \"term frequency- inverse document frequency\". According to Wikipedia:\n",
    "\n",
    "> In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Tf-idf is one of the most popular term-weighting schemes today; 83% of text-based recommend-er systems in digital libraries use tf-idf.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this calculation is to describe the relative importance of each word in a collection of documents based on its frequency of occurence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tf_idf = TfidfVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}', max_features = 5000)\n",
    "\n",
    "x_tf_idf = tf_idf.fit_transform(df['x'])\n",
    "\n",
    "x_tf_idf = pd.DataFrame(x_tf_idf.toarray(), columns = tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, our data has been processed and is ready to be fed into a model. The only remaining task is to re-label the hotels in terms of numbers rather than string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the hotel names to numbers for model\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "df['y1'] = encoder.fit_transform(df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning-\n",
    "\n",
    "Now that the hard part is over, its time to get to the fun stuff- building models. The truth is that since we have pre-processed our data: we can now feed it into virtually any machine learning algorithm you can think of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(x, df['y1'])\n",
    "\n",
    "tf_train_x, tf_test_x, tf_train_y, tf_test_y = model_selection.train_test_split(x_tf_idf, df['y1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we will limit the possibilities to a Random Forest and a Naive Bayes Classifier. \n",
    "\n",
    "However, we will first create a function that will take our data and model as inputs and return the accuracy. Thus, we could theoretically run any/all potential models. We can also reuse this code in other projects for the same purpose.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the model\n",
    "def build_model(model, x_training, y_training, x_testing, y_testing):\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(x_training, y_training)\n",
    "    \n",
    "    # Predictions\n",
    "    preds = model.predict(x_testing)\n",
    "    \n",
    "    # Output \n",
    "    return(metrics.accuracy_score(preds, y_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to create a Random Forest\n",
    "rf_count_acc = build_model(RandomForestClassifier(), train_x, train_y, test_x, test_y)\n",
    "\n",
    "rf_tf_acc = build_model(RandomForestClassifier(), tf_train_x, tf_train_y, tf_test_x, tf_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to create a Naive Bayes \n",
    "nb_count_acc = build_model(naive_bayes.MultinomialNB(), train_x, train_y, test_x, test_y)\n",
    "\n",
    "nb_tf_acc = build_model(naive_bayes.MultinomialNB(), tf_train_x, tf_train_y, tf_test_x, tf_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion- \n",
    "\n",
    "Congratulations! \n",
    "\n",
    "We have successfully completed our first forray into the complicated, confusing world of natural language processing. \n",
    "\n",
    "The skills we have learned today taught us many components of general programming in the python language, more importantly- we also learned the many steps required in order to translate and interpret textual information using machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest model accuracy for the vectorized reviews is: 0.1932501670750724\n",
      "The naive bayes model accuracy for the vectorized reviews is: 0.21753174426375585\n",
      "The random forest model accuracy for the tf-idf reviews is: 0.0797505012252172\n",
      "The naive bayes model accuracy for the tf-idf reviews is: 0.0797505012252172\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"The random forest model accuracy for the vectorized reviews is:\", rf_count_acc)\n",
    "\n",
    "print(\"The naive bayes model accuracy for the vectorized reviews is:\", rf_tf_acc)\n",
    "\n",
    "print(\"The random forest model accuracy for the tf-idf reviews is:\", nb_count_acc)\n",
    "\n",
    "print(\"The naive bayes model accuracy for the tf-idf reviews is:\", nb_count_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
