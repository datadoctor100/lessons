{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful Data Science Event:\n",
    "## Predicting the Oscars- \n",
    "### 5/29/2018\n",
    "\n",
    "In this practical workshop you'll use a dataset that contains previous Oscar winners to build a prediction model to guess the winner for Best Picture Award. In the process, you'll get an introduction to the data scientist's tools and methods. This will include an overview of basic machine learning concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started-\n",
    "\n",
    "Before we can actually start making predictions, there are several steps that we must first take to load and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory\n",
    "path = '/Documents/Work/Thinkful/Code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zansadiq/Documents/Work/Thinkful/Code\n"
     ]
    }
   ],
   "source": [
    "# Check current directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Change directory if needed\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is stored online in the format of a .zip file. This is the [link](https://www.thinkful.com/workshops/washington-dc/oscars-prediction/). You can either download the files and import them, or you can feed the information directly into python as we do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location\n",
    "url = 'https://www.dropbox.com/s/shg31hm4voydqnl/Thinkful%20Workshops%20-%20Predicting%20the%20Oscars.zip?dl=1'\n",
    "\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a staging directory\n",
    "staging_dir = \"staging\"\n",
    "os.mkdir(staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the staging directory path\n",
    "os.path.isdir(staging_dir)\n",
    "\n",
    "# Machine independent path to create new files\n",
    "zip_file = os.path.join(staging_dir, \"Thinkful Workshops - Predicting the Oscars.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file to the computer\n",
    "zf = open(zip_file,\"wb\")\n",
    "zf.write(r.content)\n",
    "zf.close()\n",
    "\n",
    "# Unzip the files\n",
    "z = zipfile.ZipFile(zip_file,\"r\")\n",
    "z.extractall(staging_dir)\n",
    "z.close()\n",
    "\n",
    "# Extract the .csv's\n",
    "files = glob.glob(os.path.join(\"staging/oscars\" + \"/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to hold the dataframes from csvs\n",
    "dict_ = {}\n",
    "\n",
    "# Write the files into the dictionary\n",
    "for file in files:\n",
    "    fname = os.path.basename(file)\n",
    "    fname = fname.replace('.csv', '')\n",
    "    dict_[fname] = pd.read_csv(file, header = 0).fillna('')\n",
    "    \n",
    "# Extract the dataframes\n",
    "train = dict_['train']\n",
    "test = dict_['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has now been downloaded, unzipped, and written into pandas data frames. The next step is to inspect and manipulate the data to make sure it is in the correct format for our machine learning algorithms. \n",
    "\n",
    "## Formatting and Cleaning-\n",
    "\n",
    "Before we can fit a model around our data, we have to make sure that certain objectives have been completed:\n",
    "\n",
    "* Inspect column types and reformat if necessary\n",
    "* Handle missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year               Movie  Won?   Budget Opening Weekend IMDB Rating  \\\n",
      "0  2016             Arrival     0  4.7e+07         2.4e+07         8.1   \n",
      "1  2016              Fences     0  2.4e+07          129462         7.5   \n",
      "2  2016       Hacksaw Ridge     0    4e+07     1.51908e+07         8.3   \n",
      "3  2016  Hell or High Water     0  1.2e+07          621329         7.7   \n",
      "4  2016      Hidden Figures     0  2.5e+07          515499         7.9   \n",
      "\n",
      "                             Genres  Won Golden Globe  Won Bafta  \\\n",
      "0  Drama, Mystery, Sci-Fi, Thriller                 0          0   \n",
      "1                             Drama                 0          0   \n",
      "2               Drama, History, War                 0          0   \n",
      "3     Action, Crime, Drama, Western                 0          0   \n",
      "4         Biography, Drama, History                 0          0   \n",
      "\n",
      "   Oscar Nominations  Golden Globe Nominations  Bafta Nominations    IMdB id  \\\n",
      "0                  8                         2                  9  tt2543164   \n",
      "1                  4                         2                  1  tt2671706   \n",
      "2                  6                         3                  5  tt2119532   \n",
      "3                  4                         3                  3  tt2582782   \n",
      "4                  3                         2                  1  tt4846340   \n",
      "\n",
      "   Won Producers  Won Directors  Won Actors   Rate Metascore  \n",
      "0              0              0           0  PG-13        81  \n",
      "1              0              0           0  PG-13        79  \n",
      "2              0              0           0      R        71  \n",
      "3              0              0           0      R        88  \n",
      "4              0              0           1     PG        74  \n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that stands out is that our target variable must be converted to a factor. Also, the string values need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to factor\n",
    "train['Won?'] = train['Won?'].astype('category')\n",
    "test['Won?'] = test['Won?'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the ratings\n",
    "train.loc[train[\"Rate\"] == \"G\", \"Rate\"] = 1\n",
    "train.loc[train[\"Rate\"] == \"PG\", \"Rate\"] = 2\n",
    "train.loc[train[\"Rate\"] == \"PG-13\", \"Rate\"] = 3\n",
    "train.loc[train[\"Rate\"] == \"R\", \"Rate\"] = 4\n",
    "\n",
    "test.loc[test[\"Rate\"] == \"G\", \"Rate\"] = 1\n",
    "test.loc[test[\"Rate\"] == \"PG\", \"Rate\"] = 2\n",
    "test.loc[test[\"Rate\"] == \"PG-13\", \"Rate\"] = 3\n",
    "test.loc[test[\"Rate\"] == \"R\", \"Rate\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check to see if we have any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                        0\n",
       "Movie                       0\n",
       "Won?                        0\n",
       "Budget                      0\n",
       "Opening Weekend             0\n",
       "IMDB Rating                 0\n",
       "Genres                      0\n",
       "Won Golden Globe            0\n",
       "Won Bafta                   0\n",
       "Oscar Nominations           0\n",
       "Golden Globe Nominations    0\n",
       "Bafta Nominations           0\n",
       "IMdB id                     0\n",
       "Won Producers               0\n",
       "Won Directors               0\n",
       "Won Actors                  0\n",
       "Rate                        0\n",
       "Metascore                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                        0\n",
       "Movie                       0\n",
       "Won?                        0\n",
       "Budget                      0\n",
       "Opening Weekend             0\n",
       "IMDB Rating                 0\n",
       "Genres                      0\n",
       "Won Golden Globe            0\n",
       "Won Bafta                   0\n",
       "Oscar Nominations           0\n",
       "Golden Globe Nominations    0\n",
       "Bafta Nominations           0\n",
       "IMdB id                     0\n",
       "Won Producers               0\n",
       "Won Directors               0\n",
       "Won Actors                  0\n",
       "Rate                        0\n",
       "Metascore                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column that has missing values is the one for `Opening Weekend`. For this reason, we can just go ahead and delete the entire column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['Opening Weekend'])\n",
    "test = test.drop(columns=['Opening Weekend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the most important steps of the process:\n",
    "\n",
    "1) Separate the dependent and independent variables\n",
    "\n",
    "2) Check the data types of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                         int64\n",
       "Movie                       object\n",
       "Budget                      object\n",
       "IMDB Rating                 object\n",
       "Genres                      object\n",
       "Won Golden Globe             int64\n",
       "Won Bafta                    int64\n",
       "Oscar Nominations            int64\n",
       "Golden Globe Nominations     int64\n",
       "Bafta Nominations            int64\n",
       "IMdB id                     object\n",
       "Won Producers                int64\n",
       "Won Directors                int64\n",
       "Won Actors                   int64\n",
       "Rate                         int64\n",
       "Metascore                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all of the column headers\n",
    "train_vars = train.columns.values.tolist()\n",
    "test_vars = test.columns.values.tolist()\n",
    "\n",
    "# Select independent variables\n",
    "x_train = [i for i in train_vars if i not in ['Won?']]\n",
    "x_test = [i for i in test_vars if i not in ['Won?']]\n",
    "\n",
    "# Fill the values and select the dependent variable\n",
    "x = train[x_train]\n",
    "y = train['Won?']\n",
    "\n",
    "x_test = test[x_test]\n",
    "y_test = test['Won?']\n",
    "\n",
    "# Column types\n",
    "x.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of our variables are listed as `object`. Before we can proceed, we must coerce these columns to numeric values so that they can be interpreted by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Fix the genres\n",
    "lb = LabelEncoder()\n",
    "\n",
    "x['Genres'] = lb.fit_transform(x['Genres'])\n",
    "x_test['Genres'] = lb.fit_transform(x_test['Genres'])\n",
    "\n",
    "# Fix the movie titles\n",
    "x['Movie'] = lb.fit_transform(x['Movie'])\n",
    "x_test['Movie'] = lb.fit_transform(x_test['Movie'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also blank cells. Because these cells are blank, they did not show up when we checked for null values. Before we can convert our remaining objects to numbers, we will have to fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the IMDB id\n",
    "x = x.drop(columns = 'IMdB id')\n",
    "x_test = x_test.drop(columns = 'IMdB id')\n",
    "\n",
    "# Fill blank cells with 0's\n",
    "x.replace(r'^\\s*$', 0, regex = True, inplace = True)\n",
    "x_test.replace(r'^\\s*$', 0, regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert remaining object variables\n",
    "cols = x.columns[x.dtypes.eq('object')]\n",
    "test_cols = x_test.columns[x_test.dtypes.eq('object')]\n",
    "\n",
    "x[cols] = x[cols].apply(pd.to_numeric)\n",
    "x_test[test_cols] = x_test[test_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning-\n",
    "\n",
    "Our data is finally in the appropriate shape. We have inspected everything to make sure that the variables are properly formatted and there are no missing values. In most projects, these steps are 90% of the data scientist's job. \n",
    "\n",
    "Now the fun part begins, we can go ahead and begin the actual building of our models. For this experiment, we will try two different methods that build upon each other. \n",
    "\n",
    "1. A simple decision tree\n",
    "2. A random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "my_tree = DecisionTreeClassifier()\n",
    "tree_fit = my_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict:\n",
    "\n",
    "We have created two different models. One for a decision tree, and the other for a random forest. It is important to note here that the random forest is an expansion on the decision tree model. \n",
    "\n",
    "A random forest is basically just a collection of decision trees where each tree is trained on a different, random subset of features included in the overall dataset. \n",
    "\n",
    "In order to evaluate these models, we will have to first make predictions and then calculate the accuracy of each one. Since we are comparing multiple models before making predictions on the test set, we will also need to split the training data into our training set, plus another set for validation. This is especially true since our test set is unlabeled, so we would otherwise have no way of measuring our accuracy before deploying the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fbc86fc13548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtree_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrf_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \"\"\"\n\u001b[1;32m    411\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1120\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "tree_preds = tree_fit.predict(x_val)\n",
    "rf_preds = rf_fit.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "Now that we have created our models and made predictions for the validation set, we can compare our algorithms to see which one is better for use on the test set.  \n",
    "\n",
    "There are many ways of assessing the predictions, such as: F1 score, specificity, sensitivity, etc. \n",
    "\n",
    "For the sake of simplicity, we will judge our models based on accuracy and also the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Validation decision tree accuracy:\", accuracy_score(y_val, tree_val_preds))\n",
    "print(\"Validation random forest accuracy:\", accuracy_score(y_val, rf_val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, our decision tree is actually slightly more accurate than our random forest. This is not uncommon, it is also the reason why Data Scientist's generally run multiple models before settling on a final selection. There is no guarentee that one model or the other will always perform better in a given set of circumstances.\n",
    "\n",
    "Lets go ahead and plot the ROC curves for both of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve: Decision Tree\n",
    "fpr, tpr, _ = roc_curve(y_val, tree_val_preds)\n",
    "tree_roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color = 'darkorange', label = 'ROC Curve (area = %0.2f)' % tree_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle = '--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: Decision Tree')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve: Random Forest\n",
    "fpr, tpr, _ = roc_curve(y_val, rf_val_preds)\n",
    "tree_roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color = 'darkorange', label = 'ROC Curve (area = %0.2f)' % tree_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle = '--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it is clear that our decision tree is actually the better model to deploy in these circumstances. \n",
    "\n",
    "All thats left to do is to make predictions on the test set, add these labels to our data, and save the output as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Predictions\n",
    "tree_test_preds = tree_fit.predict(x_test)\n",
    "\n",
    "# Add predictions to data\n",
    "x_test['Won_Preds'] = tree_test_preds\n",
    "\n",
    "# Save output\n",
    "x_test.to_csv('oscar_test_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion-\n",
    "\n",
    "In this workshop we have gone over several important concepts in data science and machine learning:\n",
    "\n",
    "* Load and transform a dataset\n",
    "* Build a model\n",
    "* Assess Predictions\n",
    "* Save output\n",
    "\n",
    "The convenient thing about the `sklearn` package in Python is that it provides a convenient, uniform interface that allows the data scientist to build a variety of different models using similar syntax. This makes it easier to deploy more models and in less time than you would otherwise. \n",
    "\n",
    "There are still many ways in which we could improve the accuracy of our model. For example, we could have imputed the missing values in the dataset instead of filling them with 0's. Also, we could have used a larger training set. Feel free to work on this at home and please feel free to reach out if you have any additional questions or concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
