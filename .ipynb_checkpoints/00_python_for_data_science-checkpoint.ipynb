{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python for Data Science:\n",
    "\n",
    "## Introduction-\n",
    "\n",
    "Data science is the hot new field in the information technology industry. Its popularity has been steadily increasing over the past few years. This increase in the sector has been caused by the information explosion that has taken place over the past few years. \n",
    "\n",
    "In fact, according to Forbes Magazine:\n",
    "\n",
    "> * Experts are predicting a 4,300 percent increase in annual data production by 2020.\n",
    "> * On average, companies use only a fraction of the data they collect and store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information explosion has been accompanied by a need for astute business analysts who are also equipped to program and build models- data scientists. Traditionally, these individuals preferred to use the R programming language to do their work. However, R is quickly being replaced by Python in this space.  \n",
    "\n",
    "We can easily visualize this growing trend of using Python for data science. For example, the popular website \"KDnuggets.com\" posted a graph that shows pythons growing usage by searching the key words used in job postings:\n",
    "\n",
    "![KDNuggets Python Graph](https://www.ibm.com/developerworks/community/blogs/jfp/resource/BLOGS_UPLOADED_IMAGES/trends0.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it is obvious that anybody looking for a hot new job in this growing field should definitely become experienced with Python in order to further their career prospects and increase their overall earning potential. \n",
    "\n",
    "Lets get to it then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started-\n",
    "\n",
    "In this article we will focus on the use of Python specifically for the purpose of carrying out data science tasks such as data cleansing and building machine learning models. As such, we will skip over the more general programming concepts and reference them only when needed to develop our specific tasks. \n",
    "\n",
    "The main components of any data science project are:\n",
    "\n",
    "**1) Import the Required Libraries**\n",
    "\n",
    "**2) Load and Manipulate the Data**\n",
    "\n",
    "**3) Build Models**\n",
    "\n",
    "**4) Compare the Results**\n",
    "\n",
    "These are the components that we will focus on in this workshop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Libraries:\n",
    "\n",
    "One of the things that makes python so powerful is that it is a free, open-source language. As a result, many talented people have created prepackaged bundles of code that can be used in our projects so that we dont have to start from scratch. This code is usually stored in the form of packages or libraries, which can easily be imported to our computer using a few simple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just imported some libraries into python that we can use to read our data and set-up our workspace.\n",
    "\n",
    "* `os` is a library that provides many functions for setting up a machine-independent workspace in python\n",
    "* `pandas` is a popular library used for reading and manipulating data stored in the form of \"data frames\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, lets set-up our workspace by defining the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zansadiq/Documents/Code/github/Thinkful\n"
     ]
    }
   ],
   "source": [
    "# Find the working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new directory\n",
    "path = 'path/to/files'\n",
    "\n",
    "# Change the working directory\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Manipulate the Data:\n",
    "\n",
    "It goes without saying that in order to carry out a data science task, we must start with some data. This can come in a variety of different formats such as `.csv` or `.xlsx`, `.json`, etc. \n",
    "\n",
    "Data can exist locally on our machine, or it may exist somewhere on the internet and need to be downloaded before it can be read. For today's tutorial, we will go ahead and use the famous \"Titanic\" dataset from the Kaggle website to play around with. The files can be downloaded [*here*](https://www.kaggle.com/c/titanic/data) and they are already split into a training and testing set for us, which is convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the data\n",
    "train = list()\n",
    "\n",
    "# Load the data from a local file using the csv module\n",
    "with open('train.csv') as titanic_train:\n",
    "    csvReader = csv.reader(titanic_train)\n",
    "    for row in csvReader:\n",
    "        train.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just used the `csv` module to load our training data into a list. A more efficient way of storing the data is in the format of a Data Frame created using the `pandas` library. \n",
    "\n",
    "We will now do two things:\n",
    "* Convert our list to a dataframe\n",
    "* Upload the test data directly into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training list to a dataframe\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6: expected 1 fields, saw 2\\nSkipping line 11: expected 1 fields, saw 8\\nSkipping line 19: expected 1 fields, saw 5\\nSkipping line 20: expected 1 fields, saw 5\\nSkipping line 28: expected 1 fields, saw 45\\nSkipping line 30: expected 1 fields, saw 2\\nSkipping line 43: expected 1 fields, saw 3\\nSkipping line 44: expected 1 fields, saw 3\\nSkipping line 45: expected 1 fields, saw 2\\nSkipping line 51: expected 1 fields, saw 7\\nSkipping line 53: expected 1 fields, saw 2\\nSkipping line 57: expected 1 fields, saw 5\\nSkipping line 59: expected 1 fields, saw 4\\nSkipping line 60: expected 1 fields, saw 2\\nSkipping line 61: expected 1 fields, saw 2\\nSkipping line 69: expected 1 fields, saw 6\\nSkipping line 103: expected 1 fields, saw 2\\nSkipping line 115: expected 1 fields, saw 2\\nSkipping line 116: expected 1 fields, saw 2\\nSkipping line 117: expected 1 fields, saw 2\\nSkipping line 118: expected 1 fields, saw 2\\nSkipping line 119: expected 1 fields, saw 2\\nSkipping line 135: expected 1 fields, saw 2\\nSkipping line 196: expected 1 fields, saw 2\\n'\n"
     ]
    }
   ],
   "source": [
    "# Load the test data directly from a url\n",
    "test = pd.read_csv('https://www.kaggle.com/c/3136/download/test.csv', error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we can see that the simplest method of loading the data is not always the most ideal. The error probably arises from the fact that the `.csv` was not created properly to begin with. While it is hard to say for sure, this is most likely an error from having the delimiter present within certain fields of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the test data using csv\n",
    "test = list()\n",
    "\n",
    "with open('test.csv') as titanic_test:\n",
    "    csvReader = csv.reader(titanic_test)\n",
    "    for row in csvReader:\n",
    "        test.append(row)\n",
    "        \n",
    "# Convert to data frame\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now uploaded our data into python and converted it into data frames. Lets go ahead and take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "      <td>Survived</td>\n",
       "      <td>Pclass</td>\n",
       "      <td>Name</td>\n",
       "      <td>Sex</td>\n",
       "      <td>Age</td>\n",
       "      <td>SibSp</td>\n",
       "      <td>Parch</td>\n",
       "      <td>Ticket</td>\n",
       "      <td>Fare</td>\n",
       "      <td>Cabin</td>\n",
       "      <td>Embarked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.25</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.925</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1       2   \\\n",
       "0  PassengerId  Survived  Pclass   \n",
       "1            1         0       3   \n",
       "2            2         1       1   \n",
       "3            3         1       3   \n",
       "4            4         1       1   \n",
       "\n",
       "                                                  3       4    5      6   \\\n",
       "0                                               Name     Sex  Age  SibSp   \n",
       "1                            Braund, Mr. Owen Harris    male   22      1   \n",
       "2  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "3                             Heikkinen, Miss. Laina  female   26      0   \n",
       "4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "\n",
       "      7                 8        9      10        11  \n",
       "0  Parch            Ticket     Fare  Cabin  Embarked  \n",
       "1      0         A/5 21171     7.25                S  \n",
       "2      0          PC 17599  71.2833    C85         C  \n",
       "3      0  STON/O2. 3101282    7.925                S  \n",
       "4      0            113803     53.1   C123         S  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above information, we can already see that we have encountered an issue with our file import. The headers have been inserted into row 0 and are in the incorrect location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.25</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.925</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.05</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 PassengerId Survived Pclass  \\\n",
       "1           1        0      3   \n",
       "2           2        1      1   \n",
       "3           3        1      3   \n",
       "4           4        1      1   \n",
       "5           5        0      3   \n",
       "\n",
       "0                                               Name     Sex Age SibSp Parch  \\\n",
       "1                            Braund, Mr. Owen Harris    male  22     1     0   \n",
       "2  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38     1     0   \n",
       "3                             Heikkinen, Miss. Laina  female  26     0     0   \n",
       "4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35     1     0   \n",
       "5                           Allen, Mr. William Henry    male  35     0     0   \n",
       "\n",
       "0            Ticket     Fare Cabin Embarked  \n",
       "1         A/5 21171     7.25              S  \n",
       "2          PC 17599  71.2833   C85        C  \n",
       "3  STON/O2. 3101282    7.925              S  \n",
       "4            113803     53.1  C123        S  \n",
       "5            373450     8.05              S  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the headers\n",
    "train.columns = train.iloc[0]\n",
    "\n",
    "test.columns = test.iloc[0]\n",
    "\n",
    "# Delete the row\n",
    "train = train.reindex(train.index.drop(0))\n",
    "\n",
    "test = test.reindex(test.index.drop(0))\n",
    "\n",
    "# Check\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, a dataset can contain more information than we actually need. In this particular situation, the columns \"Ticket\" and \"Cabin\" dont really provide any useful additional information. More importantly, they also have numerous missing values. \n",
    "\n",
    "The missing values will cause problems for us when we try to build a model because the machine learning algorithms cannot handle these cells. \n",
    "\n",
    "To address these issues, we will first drop the unnecessary columns and then check the remaining data for additional missing information. \n",
    "\n",
    "*Note- we are also going to delete the \"Name\" variable because this is a string and we can already identify passengers by id.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "train = train.drop(columns = ['Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "\n",
    "test = test.drop(columns = ['Name', 'Ticket', 'Cabin'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId    0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so there are no additional missing values. This means we can go ahead and move on. \n",
    "\n",
    "The next step is to make sure that each variable is encoded properly and that the data types are all appropriate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId    object\n",
       "Survived       object\n",
       "Pclass         object\n",
       "Sex            object\n",
       "Age            object\n",
       "SibSp          object\n",
       "Parch          object\n",
       "Fare           object\n",
       "Embarked       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition asks us to predict whether or not passengers survived. Lets go ahead and explicitly encode our variables so that they are in the desired formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert factor variables\n",
    "train['Survived'] = train['Survived'].astype('category')\n",
    "train['Pclass'] = train['Pclass'].astype('category')\n",
    "train['Sex'] = train['Sex'].astype('category')\n",
    "train['Embarked'] = train['Embarked'].astype('category')\n",
    "\n",
    "test['Pclass'] = test['Pclass'].astype('category')\n",
    "test['Sex'] = test['Sex'].astype('category')\n",
    "test['Embarked'] = test['Embarked'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert remaining variables to numeric\n",
    "train['PassengerId'] = pd.to_numeric(train['PassengerId'])\n",
    "train['Age'] = pd.to_numeric(train['Age'])\n",
    "train['SibSp'] = pd.to_numeric(train['SibSp'])\n",
    "train['Parch'] = pd.to_numeric(train['Parch'])\n",
    "train['Fare'] = pd.to_numeric(train['Fare'])\n",
    "\n",
    "test['PassengerId'] = pd.to_numeric(test['PassengerId'])\n",
    "test['Age'] = pd.to_numeric(test['Age'])\n",
    "test['SibSp'] = pd.to_numeric(test['SibSp'])\n",
    "test['Parch'] = pd.to_numeric(test['Parch'])\n",
    "test['Fare'] = pd.to_numeric(test['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Fare             0\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check for null values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId     0\n",
       "Pclass          0\n",
       "Sex             0\n",
       "Age            86\n",
       "SibSp           0\n",
       "Parch           0\n",
       "Fare            1\n",
       "Embarked        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check for null values\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our conversion of the variables from objects to numbers seems to have introduced some `NA`s. This could have occurred for several reasons. For example, those values could have been blank cells. Perhaps typos introduced non-numeric characters. Whatever the case may be- we must derive a way to handle these values.  \n",
    "\n",
    "The general rule of thumb within the data science community is that null values can be deleted if they account for less than 5% of the data. In our case, the number is slightly higher. We will have to impute the missing cells. There are packages that can perform these imputations. We will rely on a simpler method of handling them by filling empty cells with averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average fare and assign this to missing value in test set\n",
    "test.loc[test.Fare.isnull(), 'Fare'] = test['Fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing age cells using pandas\n",
    "train['Age'].fillna(train['Age'].mean(), inplace = True)\n",
    "\n",
    "test['Age'].fillna(test['Age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure the values have been filled\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "PassengerId    0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to split the data. However, we need to make on additional change before we can do that. The columns for \"Embarked\" and \"Sex\" have values that represent categories using letters or words. We need to convert these to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Embarked\" column\n",
    "lb = LabelEncoder()\n",
    "\n",
    "train['Embarked'] = lb.fit_transform(train['Embarked'])\n",
    "test['Embarked'] = lb.fit_transform(test['Embarked'])\n",
    "\n",
    "train['Sex'] = lb.fit_transform(train['Sex'])\n",
    "test['Sex'] = lb.fit_transform(test['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data-\n",
    "\n",
    "We have been provided with a training set and a testing set. In some situations, we could simply use the training data to make predictions for the test set. However, it is good practice to run multiple models at once. In order to evaluate these models against one another- we will have to further split our training data into a validation set as well. \n",
    "\n",
    "In order to split the data appropriately, we will first separate our columns into x and y (features and target variable). This will only be done for the training data because the \"Survived\" column is not present in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all of the column headers\n",
    "train_vars = train.columns.values.tolist()\n",
    "\n",
    "# Select independent variables\n",
    "x_train = [i for i in train_vars if i not in ['Survived']]\n",
    "\n",
    "# Fill the values and select the dependent variable\n",
    "x = train[x_train]\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to numbers\n",
    "x = x.apply(pd.to_numeric)\n",
    "y = y.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models\n",
    "\n",
    "Finally, we are ready to begin the process of actually building some models. There are so many to choose from. Part of a data scientist's job is knowing which models to deploy in which scenarios, and when. As it was mentioned earlier- it is usually a good idea to go ahead and try running multiple models for a given set of data because there is no clear distinction between the majority of algorithms as far as performance with regard to a given set of data. \n",
    "\n",
    "* In any case, our task will be to create some models and fit them to the training data.\n",
    "    * Then, we will compare their accuracy on the validation set.\n",
    "        * Finally, we will make predictions on the test set using our winning algorithm.\n",
    "        \n",
    "Once we have done all of this, we will add our predictions to the test set by initializing a \"Survived\" column and writing in the predicted values. The output will be saved to a `.csv` file which we can then upload to Kaggle in order to gauge our final outcome. \n",
    "\n",
    "#### Initializing the Algorithms-\n",
    "\n",
    "There are many options in terms of packages and algorithms to use. One of the most popular libraries for data science in python is the `SKLearn` package. This library provides a convenient interface and syntax for building and deploying a variety of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree\n",
    "my_tree = DecisionTreeClassifier()\n",
    "tree_fit = my_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr_fit = lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions on the Validation Set-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "tree_preds = tree_fit.predict(x_val)\n",
    "rf_preds = rf_fit.predict(x_val)\n",
    "lr_preds = lr.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Results:\n",
    "\n",
    "#### Assess the Models-\n",
    "\n",
    "Now that we have made our models and predictions, it is time to assess them. This can be done in one of several ways. \n",
    "\n",
    "This problem is unique in that it is \"one-class\" (the target variable is binary with an outcome of 0 or 1).\n",
    "\n",
    "A common means of assessing a \"one-class\" model is using an ROC curve. This is a plot of specificity vs. sensitivity for the predictions. \n",
    "\n",
    "* **Specificity**- True Positive Rate\n",
    "* **Sensitivity**- True Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main point is that, when you have an ROC curve for a given class- the larger \"area under the curve\" (AUC) is an indicator of a better model. Alternatively, we can also use pure accuracy as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set, decision tree accuracy: 0.7574626865671642\n",
      "Validation set, random forest accuracy: 0.8171641791044776\n",
      "Validation set, logistic regression accuracy: 0.7835820895522388\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Validation set, decision tree accuracy:\", accuracy_score(y_val, tree_preds))\n",
    "print(\"Validation set, random forest accuracy:\", accuracy_score(y_val, rf_preds))\n",
    "print(\"Validation set, logistic regression accuracy:\", accuracy_score(y_val, lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that the Random Forest model is the most accurate. Lets also take a look at the AUC values for each model to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for the Decision Tree is 0.7465524205181467\n"
     ]
    }
   ],
   "source": [
    "# ROC Curve: Decision Tree\n",
    "fpr, tpr, _ = roc_curve(y_val, tree_preds)\n",
    "tree_roc_auc = auc(fpr, tpr)\n",
    "print(\"The AUC for the Decision Tree is\", tree_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for the Random Forest is 0.7954243840516992\n"
     ]
    }
   ],
   "source": [
    "# ROC Curve: Random Forest\n",
    "fpr, tpr, _ = roc_curve(y_val, rf_preds)\n",
    "rf_roc_auc = auc(fpr, tpr)\n",
    "print(\"The AUC for the Random Forest is\", rf_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for the Logistic Regression is 0.7613524897582368\n"
     ]
    }
   ],
   "source": [
    "# ROC Curve: Logistic Regression\n",
    "fpr, tpr, _ = roc_curve(y_val, lr_preds)\n",
    "lr_roc_auc = auc(fpr, tpr)\n",
    "print(\"The AUC for the Logistic Regression is\", lr_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based upon our results it would appear as if the most reliable model is the random forest. Now, what we can do is upload our predictions on the test to Kaggle: this will give us a final assessment of our model because Kaggle will score the predictions for the unlabeled test set and give us our final accuracy.\n",
    "\n",
    "#### Write the Results to CSV-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill a new column in the test data for the predictions\n",
    "test['Survived'] = rf_fit.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df for the results\n",
    "out = pd.DataFrame(test['PassengerId'])\n",
    "out['Survived'] = test['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways that we can make the final output: using pandas or base python code. Both formats are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to PassengerId for submission\n",
    "out.set_index('PassengerId', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to create the file\n",
    "out.to_csv('kaggle_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name and header\n",
    "csv_header = 'PassengerId, Survived' \n",
    "file_name = 'kaggle_submission.csv'\n",
    "\n",
    "# Function to create the csv\n",
    "def print_results(file_name, csv_header, data): \n",
    "    with open(file_name,'wt') as f:\n",
    "        print(csv_header, file = f) \n",
    "        for s in data:\n",
    "            print(','.join(s), file = f)\n",
    "\n",
    "# Call function\n",
    "print_results(file_name, csv_header, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle Submission\n",
    "\n",
    "Well, it looks like our accuracy on the test set was roughly equal to that of the validation data. This results in a score of .74 on Kaggle. Feel free to try and run some other models at home, or use feature engineering/selection to see if you cant make any improvements on our final score. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Submission](kaggle_submission.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
